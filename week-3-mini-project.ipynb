{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/michport/cancer-detection-project?scriptVersionId=243116570\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Project: Histopathologic Cancer Detection\n\nThis notebook provides a solution for the Histopathologic Cancer Detection Kaggle competition, developed as part of the Deep Learning course at CU Boulder. It includes problem explanation, exploratory data analysis (EDA), data preprocessing, model building with hyperparameter tuning, results analysis, and a conclusion.\n\n## 1. Project Explanation\nThe goal is to classify histopathologic images as cancerous (label=1) or non-cancerous (label=0) based on a 32x32 pixel region in 96x96 pixel images. The dataset contains over 220,000 training images and 57,000 test images, each in .tif format. The task is a binary classification problem, requiring a deep learning model to detect cancerous regions accurately.\n\n## 2. Data Description\nThe dataset includes:\n- **Training labels**: A CSV file (`train_labels.csv`) with image IDs and binary labels (0 or 1).\n- **Training images**: 96x96 pixel .tif images in the `train` directory.\n- **Test images**: 96x96 pixel .tif images in the `test` directory, without labels.\n- **Size**: ~220,000 training images, ~57,000 test images.\n- **Structure**: Images are RGB, and labels indicate the presence (1) or absence (0) of cancer in the central 32x32 region.\n\n## 3. Imports and exploratory Data Analysis (EDA)","metadata":{"_uuid":"07f78c6f-4778-4894-a4ee-501456e3ffbd","_cell_guid":"5f50226f-43cf-4fff-aa1c-2d587374cf3b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport warnings\nfrom collections import defaultdict\n\nwarnings.filterwarnings('ignore')\n\n# Define paths\ndata_dir = '/kaggle/input/histopathologic-cancer-detection/'\ntrain_labels_path = os.path.join(data_dir, 'train_labels.csv')\ntrain_images_dir = os.path.join(data_dir, 'train')\ntest_images_dir = os.path.join(data_dir, 'test')\n\n# Load labels\ntrain_labels_df = pd.read_csv(train_labels_path)\n\n# Basic data description\nprint(f\"Training labels shape: {train_labels_df.shape}\")\nprint(\"\\nFirst 5 rows of training labels:\")\nprint(train_labels_df.head().to_markdown(index=False))\nprint(\"\\nMissing values in training labels:\")\nprint(train_labels_df.isnull().sum())\n\n# Check for duplicates\nprint(\"\\nDuplicate rows in training labels:\")\nprint(train_labels_df[train_labels_df.duplicated(keep=False)])\n\n# Class distribution\nclass_counts = train_labels_df['label'].value_counts()\nprint(\"\\nClass distribution:\")\nprint(class_counts)\n\n# Visualize class distribution\nplt.figure(figsize=(8, 6))\nsns.countplot(x='label', data=train_labels_df)\nplt.title('Class Distribution (0: Non-Cancerous, 1: Cancerous)')\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.show()\n\n# Display sample images\ndef display_sample_images(df, image_dir, num_samples=3):\n    fig, axes = plt.subplots(2, num_samples, figsize=(num_samples*4, 8))\n    for i, label in enumerate([0, 1]):\n        label_df = df[df['label'] == label].sample(num_samples, random_state=42)\n        for j, row in enumerate(label_df.itertuples()):\n            img_path = os.path.join(image_dir, f'{row.id}.tif')\n            img = Image.open(img_path)\n            axes[i, j].imshow(img)\n            axes[i, j].set_title(f'Label: {label}')\n            axes[i, j].axis('off')\n    plt.suptitle('Sample Images (Top: Non-Cancerous, Bottom: Cancerous)')\n    plt.show()\n\ndisplay_sample_images(train_labels_df, train_images_dir)\n\n# Image statistics\nsample_image = Image.open(os.path.join(train_images_dir, f'{train_labels_df[\"id\"].iloc[0]}.tif'))\nprint(f\"\\nSample image dimensions: {sample_image.size}\")\nprint(f\"Sample image mode: {sample_image.mode}\")\n\n# Pixel intensity distribution (histograms for multiple samples)\ndef plot_pixel_intensity(image_dir, sample_ids, title):\n    fig, axes = plt.subplots(1, len(sample_ids), figsize=(4 * len(sample_ids), 5))\n    if len(sample_ids) == 1: # Handle single subplot case\n        axes = [axes]\n    for i, img_id in enumerate(sample_ids):\n        img_path = os.path.join(image_dir, f'{img_id}.tif')\n        img = np.array(Image.open(img_path))\n        axes[i].hist(img.flatten(), bins=50, color='skyblue', alpha=0.7)\n        axes[i].set_title(f'Image {img_id[:5]}')\n        axes[i].set_xlabel('Pixel Intensity')\n        axes[i].set_ylabel('Frequency')\n    plt.suptitle(title, y=1.02)\n    plt.tight_layout()\n    plt.show()\n\nsample_ids = train_labels_df['id'].sample(3, random_state=42).values\nplot_pixel_intensity(train_images_dir, sample_ids, 'Pixel Intensity Distribution for Sample Images')\n\n# Check pixel value range (min/max)\nmin_pixel_values = []\nmax_pixel_values = []\nsample_ids_check = train_labels_df['id'].sample(50, random_state=42).values # Check more samples for range\nfor img_id in sample_ids_check:\n    img_path = os.path.join(train_images_dir, f'{img_id}.tif')\n    img = np.array(Image.open(img_path))\n    min_pixel_values.append(np.min(img))\n    max_pixel_values.append(np.max(img))\n\nprint(f\"\\nMin pixel value across {len(sample_ids_check)} samples: {np.min(min_pixel_values)}\")\nprint(f\"Max pixel value across {len(sample_ids_check)} samples: {np.max(max_pixel_values)}\")\n\n# Summarize image similarity (simple approach: average pixel values or standard deviation)\n# This is a very basic form of similarity, more advanced methods would use embeddings or perceptual hashing.\navg_pixel_values_by_label = defaultdict(list)\nfor index, row in train_labels_df.sample(100, random_state=42).iterrows(): # Sample 100 images for this\n    img_path = os.path.join(train_images_dir, f'{row[\"id\"]}.tif')\n    img = np.array(Image.open(img_path))\n    avg_pixel_values_by_label[row['label']].append(np.mean(img))\n\nprint(\"\\nAverage pixel values for sample images by class:\")\nfor label, values in avg_pixel_values_by_label.items():\n    print(f\"Label {label}: Mean={np.mean(values):.2f}, Std Dev={np.std(values):.2f}\")\n\n# Check image sizes (confirming they are all 96x96 as stated)\nunique_image_sizes = set()\nfor i, img_id in enumerate(train_labels_df['id'].sample(100, random_state=42).values): # Check a subset\n    img_path = os.path.join(train_images_dir, f'{img_id}.tif')\n    with Image.open(img_path) as img:\n        unique_image_sizes.add(img.size)\n    if i > 100: # Limit check to prevent slow execution\n        break\n\nprint(f\"\\nUnique image sizes observed: {unique_image_sizes}\")\nif len(unique_image_sizes) == 1 and list(unique_image_sizes)[0] == (96, 96):\n    print(\"All sample images conform to the expected 96x96 dimensions.\")\nelse:\n    print(\"Image sizes vary or are not all 96x96.\")","metadata":{"_uuid":"e7e5a4dd-ffa0-40d0-a5b2-c0a4a031d775","_cell_guid":"5b5425b4-c375-4748-8628-d3f0aaf74a21","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Interpretation of EDA\nThe Exploratory Data Analysis provides crucial insights into the dataset, guiding our subsequent data preprocessing and model development.\n\n1.  **Dataset Overview**: The training set contains 220,025 images, each with a corresponding label. There are no missing values or duplicate entries in the `train_labels.csv` file, indicating a clean and well-structured label dataset.\n\n2.  **Class Distribution**: I observed a class imbalance, with approximately 60% of images classified as non-cancerous (label 0) and 40% as cancerous (label 1). Specifically, there are 130,908 non-cancerous images and 89,117 cancerous images. This imbalance is significant and suggests that simply optimizing for accuracy might lead to a model biased towards the majority class. Therefore, using **class weights** in the loss function during training is a sensible strategy to mitigate this bias and ensure the model learns effectively from both classes.\n\n3.  **Sample Image Visualization**: Displaying sample images for both non-cancerous and cancerous classes helps in understanding the visual characteristics of the data. While subtle, cancerous regions often appear as darker, more chaotic patches within the tissue, contrasting with the more uniform appearance of non-cancerous tissue. This visual inspection confirms the complexity of the task and the need for robust feature extraction by the CNN.\n\n4.  **Image Dimensions and Mode**: All images are confirmed to be 96x96 pixels and in RGB mode (3 channels). This uniformity simplifies data loading and avoids the need for resizing or channel conversion, making the input consistent for the convolutional neural network.\n\n5.  **Pixel Intensity Distribution**: Histograms of pixel intensities for sample images show a broad distribution, typically ranging from 0 to 255. The distributions generally appear somewhat similar across different images, suggesting a consistent dynamic range of pixel values. The minimum pixel values are consistently near 0, and maximum pixel values are near 255, indicating that the images fully utilize the 8-bit per channel range. This consistency implies that a standard normalization scheme (like ImageNet mean/std) should be effective across the dataset.\n\n6.  **Image Similarity (Basic)**: A basic check of average pixel values across a sample of images for each class shows slight differences in mean pixel values, but more importantly, a relatively consistent standard deviation within each class. This very simple metric doesn't provide deep insights into visual similarity but suggests that images within the same class generally have similar overall brightness and contrast characteristics. More sophisticated similarity analyses would involve feature embeddings from a pre-trained model.\n\nIn summary, the EDA highlights the binary classification nature of the problem, the importance of addressing class imbalance, the uniform image dimensions, and the general consistency in pixel value ranges. These findings directly inform the choice of data augmentation, normalization, loss function weighting, and model architecture.\n\n---\n## 4. Data Preprocessing & Analysis Plan\nBased on EDA:\n- **Class imbalance**: ~60% non-cancerous, ~40% cancerous. Use class weights in loss function.\n- **Preprocessing**: Normalize images using mean and std from ImageNet. Apply data augmentation (flips, rotations) to improve generalization.\n- **Plan**: Use a CNN with iterative hyperparameter tuning (learning rate, batch size, architecture depth). Evaluate using validation accuracy and AUC.","metadata":{"_uuid":"55053f6b-033c-4a19-bc14-4cbf10c474ce","_cell_guid":"6368565b-280c-4f45-b152-766c76b7ac3b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Enhanced data transformations\ntrain_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(), # Added vertical flip\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), # Added color jitter\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Custom dataset\nclass CancerDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, image_dir, transform=None):\n        self.dataframe = dataframe\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = self.dataframe.iloc[idx, 0]\n        label = self.dataframe.iloc[idx, 1] if 'label' in self.dataframe.columns else 0\n        img_path = os.path.join(self.image_dir, f'{img_name}.tif')\n        image = Image.open(img_path)\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# Split dataset\nfrom sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(train_labels_df, test_size=0.2, stratify=train_labels_df['label'], random_state=42)\n\n# Create datasets and loaders\ntrain_dataset = CancerDataset(train_df, train_images_dir, transform=train_transforms)\nval_dataset = CancerDataset(val_df, train_images_dir, transform=val_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)","metadata":{"_uuid":"c0295dfd-9e5a-4bbb-9f7f-16fd348f9402","_cell_guid":"3b275d48-cd18-4d73-b083-7e286c1b0cbb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Model Architecture\nI use a deeper CNN with batch normalization and dropout for regularization. The architecture is inspired by VGG-like models but tailored for 96x96 images.","metadata":{"_uuid":"3532819f-f8e6-4201-89d7-335839922b1a","_cell_guid":"2ba102b5-d55c-4e4e-b940-494ea951b3ed","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass DeepCNN(nn.Module):\n    def __init__(self, num_filters=32, dropout_rate=0.5):\n        super(DeepCNN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, num_filters, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_filters),\n            nn.ReLU(),\n            nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_filters),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # Output size: 48x48\n\n            nn.Conv2d(num_filters, num_filters*2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_filters*2),\n            nn.ReLU(),\n            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_filters*2),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # Output size: 24x24\n\n            nn.Conv2d(num_filters*2, num_filters*4, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_filters*4),\n            nn.ReLU(),\n            nn.Conv2d(num_filters*4, num_filters*4, kernel_size=3, padding=1), # Added another conv layer\n            nn.BatchNorm2d(num_filters*4),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # Output size: 12x12\n\n            nn.Conv2d(num_filters*4, num_filters*8, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_filters*8),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # Output size: 6x6\n        )\n        with torch.no_grad():\n            dummy_input = torch.zeros(1, 3, 96, 96)\n            dummy_output = self.features(dummy_input)\n            self.flattened_size = dummy_output.view(1, -1).size(1)\n        self.classifier = nn.Sequential(\n            nn.Linear(self.flattened_size, 512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 128), # Added another dense layer\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(128, 2)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x","metadata":{"_uuid":"f136fb9b-e97f-4bcf-98d6-4bc958399018","_cell_guid":"e6421661-c8d9-4d44-90e2-85c29f04b486","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameter tuning with early stopping\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=10, patience=3): # Increased patience\n    train_losses, val_losses, val_accuracies, val_aucs = [], [], [], []\n    best_auc, best_model = 0.0, None\n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n\n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        train_loss = running_loss / len(train_loader)\n        train_losses.append(train_loss)\n\n        # Validation\n        model.eval()\n        val_loss, correct, total, all_preds, all_labels, all_probs = 0.0, 0, 0, [], [], []\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                correct += (predicted == labels).sum().item()\n                total += labels.size(0)\n                probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                all_probs.extend(probs)\n\n        val_loss /= len(val_loader)\n        val_accuracy = 100 * correct / total\n        val_auc = roc_auc_score(all_labels, all_probs)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_accuracy)\n        val_aucs.append(val_auc)\n        scheduler.step(val_loss)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, \"\n              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, Val AUC: {val_auc:.4f}\")\n\n        if val_auc > best_auc:\n            best_auc = val_auc\n            best_model = model.state_dict()\n            # Reset patience counter only if AUC improves significantly (e.g., > 0.001)\n            epochs_no_improve = 0 # Reset on any improvement\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation AUC for {patience} epochs.\")\n                break\n    return train_losses, val_losses, val_accuracies, val_aucs, best_model, all_labels, all_preds","metadata":{"_uuid":"4fe9366d-1817-4d99-bcd4-8d80d904de73","_cell_guid":"b7d2acde-20f4-462c-874d-2f60c3ccaaa3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Additional Note\nI realized, that with learning rate 0.001 I had an overfit after 5 pochs, so i added the patience parameter, which checks if there is still imrpovement and stops otherwise.","metadata":{"_uuid":"69c1c7b4-51e0-40b1-8f41-67e3a5770abc","_cell_guid":"a64de429-9733-471e-ae12-9d10f76bed43","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Try different hyperparameters\nhyperparams = [\n    {'num_filters': 32, 'dropout_rate': 0.5, 'lr': 0.001, 'batch_size': 64, 'epochs': 15}, # Increased epochs for potentially better convergence\n    {'num_filters': 64, 'dropout_rate': 0.4, 'lr': 0.0008, 'batch_size': 32, 'epochs': 15}, # Adjusted dropout and LR\n    {'num_filters': 48, 'dropout_rate': 0.5, 'lr': 0.001, 'batch_size': 128, 'epochs': 15} # New set of parameters\n]\n\nresults = []\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Calculate class weights for imbalanced dataset\nclass_counts = train_labels_df['label'].value_counts()\n# Weight for class 0 (non-cancerous) = Total samples / (2 * Count of class 0)\n# Weight for class 1 (cancerous) = Total samples / (2 * Count of class 1)\n# A more common approach is to use the inverse of the class frequencies normalized,\n# or simply max_count / class_count for each class.\n# Here, I use max_count / count_of_each_class for simplicity in PyTorch's CrossEntropyLoss\nweight_for_0 = class_counts[1] / class_counts[0]\nweight_for_1 = 1.0 # Or class_counts[0] / class_counts[1] if 0 is the minority class. Here, 1 is the minority class.\n# Given class 0 is majority (130908) and class 1 is minority (89117)\n# I want to give higher weight to the minority class.\nclass_weights = torch.tensor([weight_for_0, weight_for_1], dtype=torch.float).to(device)\n# Alternatively, consider balanced weights: N_samples / (N_classes * N_samples_class_i)\ntotal_samples = len(train_labels_df)\nclass_0_weight = total_samples / (2 * class_counts[0])\nclass_1_weight = total_samples / (2 * class_counts[1])\nclass_weights = torch.tensor([class_0_weight, class_1_weight], dtype=torch.float).to(device)\nprint(f\"Calculated class weights: {class_weights.cpu().numpy()}\")\n\n\nfor i, params in enumerate(hyperparams):\n    print(f\"\\n--- Training with Hyperparameter Set {i+1}: {params} ---\")\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, num_workers=4)\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=4)\n    model = DeepCNN(num_filters=params['num_filters'], dropout_rate=params['dropout_rate']).to(device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights) # Apply class weights\n    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n    train_losses, val_losses, val_accuracies, val_aucs, best_model, all_labels, all_preds = train_and_evaluate(\n        model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=params['epochs']\n    )\n    results.append({\n        'params': params,\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_accuracies': val_accuracies,\n        'val_aucs': val_aucs,\n        'best_model': best_model,\n        'final_val_labels': all_labels,\n        'final_val_preds': all_preds\n    })","metadata":{"_uuid":"9c1cfb1a-e966-4297-9faf-f8fbe01f41c4","_cell_guid":"d647b6de-ae55-4940-93e1-2a4fbd7afa37","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 6. Results & Analysis\nThis section details the outcomes of the model training with different hyperparameter configurations and provides an analysis of their performance.","metadata":{"_uuid":"a79c0981-0269-4cdd-830e-9539ef74bc1c","_cell_guid":"f936a969-3245-4cf2-a90a-4b28f740f9da","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Plot results\nplt.figure(figsize=(18, 12))\n\nfor i, result in enumerate(results):\n    params_str = \", \".join([f\"{k}:{v}\" for k, v in result['params'].items()])\n\n    # Plot Training and Validation Loss\n    plt.subplot(2, 3, 1)\n    plt.plot(result['train_losses'], label=f\"Train Loss (Set {i+1})\")\n    plt.plot(result['val_losses'], label=f\"Val Loss (Set {i+1})\")\n    plt.title('Training and Validation Loss', fontsize=14)\n    plt.xlabel('Epoch', fontsize=12)\n    plt.ylabel('Loss', fontsize=12)\n    plt.legend(loc='upper right', fontsize=10)\n    plt.grid(True, linestyle='--', alpha=0.6)\n\n    # Plot Validation Accuracy\n    plt.subplot(2, 3, 2)\n    plt.plot(result['val_accuracies'], label=f\"Val Accuracy (Set {i+1})\")\n    plt.title('Validation Accuracy', fontsize=14)\n    plt.xlabel('Epoch', fontsize=12)\n    plt.ylabel('Accuracy (%)', fontsize=12)\n    plt.legend(loc='lower right', fontsize=10)\n    plt.grid(True, linestyle='--', alpha=0.6)\n\n    # Plot Validation AUC\n    plt.subplot(2, 3, 3)\n    plt.plot(result['val_aucs'], label=f\"Val AUC (Set {i+1})\")\n    plt.title('Validation AUC', fontsize=14)\n    plt.xlabel('Epoch', fontsize=12)\n    plt.ylabel('AUC', fontsize=12)\n    plt.legend(loc='lower right', fontsize=10)\n    plt.grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()\n\n# Display detailed results for each hyperparameter set\nprint(\"\\n--- Detailed Results per Hyperparameter Set ---\")\nfor i, result in enumerate(results):\n    params = result['params']\n    final_val_accuracy = result['val_accuracies'][-1] if result['val_accuracies'] else 'N/A'\n    final_val_auc = result['val_aucs'][-1] if result['val_aucs'] else 'N/A'\n    print(f\"\\nHyperparameter Set {i+1}:\")\n    print(f\"  Parameters: {params}\")\n    print(f\"  Final Validation Accuracy: {final_val_accuracy:.2f}%\")\n    print(f\"  Final Validation AUC: {final_val_auc:.4f}\")\n\n    # Display Confusion Matrix and Classification Report for the last epoch's predictions\n    if result['final_val_labels'] and result['final_val_preds']:\n        cm = confusion_matrix(result['final_val_labels'], result['final_val_preds'])\n        print(\"\\n  Confusion Matrix:\")\n        print(pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1']).to_markdown())\n\n        print(\"\\n  Classification Report:\")\n        print(classification_report(result['final_val_labels'], result['final_val_preds'], target_names=['Non-Cancerous (0)', 'Cancerous (1)']))\n\n        plt.figure(figsize=(6, 5))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                    xticklabels=['Predicted 0', 'Predicted 1'],\n                    yticklabels=['Actual 0', 'Actual 1'])\n        plt.title(f'Confusion Matrix for Set {i+1}')\n        plt.xlabel('Predicted Label')\n        plt.ylabel('True Label')\n        plt.show()\n\n# Hyperparameter Optimization Summary\nprint(\"\\n--- Hyperparameter Optimization Summary ---\")\nbest_overall_auc = 0.0\nbest_overall_params = None\nbest_overall_model_state = None\n\nfor i, result in enumerate(results):\n    current_best_auc_for_set = max(result['val_aucs'])\n    print(f\"Set {i+1} (Params: {result['params']}): Max Val AUC = {current_best_auc_for_set:.4f}\")\n    if current_best_auc_for_set > best_overall_auc:\n        best_overall_auc = current_best_auc_for_set\n        best_overall_params = result['params']\n        best_overall_model_state = result['best_model']\n\nprint(f\"\\nBest performing model parameters: {best_overall_params}\")\nprint(f\"Achieved best Validation AUC: {best_overall_auc:.4f}\")","metadata":{"_uuid":"5577c765-14f7-4466-945b-1ef199522256","_cell_guid":"6638a17e-0885-41e9-8f5d-2412123a2ee6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analysis of Model Performance\n### Observations from the Training Process\n\n**Early Stopping Effectiveness**: The patience parameter in ReduceLROnPlateau and the early stopping mechanism successfully prevented overfitting, especially evident when the validation loss started to plateau or increase. This ensured that models were saved at their optimal performance point on the validation set, typically when AUC was maximized.\n\n**Impact of Learning Rate**: The initial learning rate (e.g., 0.001) in some configurations showed rapid initial improvement followed by a plateau or slight increase in validation loss, suggesting a need for learning rate scheduling. The ReduceLROnPlateau scheduler effectively reduced the learning rate when the validation loss stagnated, allowing for finer adjustments and continued, albeit slower, convergence.\n\n**Batch Size Influence**: Larger batch sizes (e.g., 64, 128) generally resulted in faster training times per epoch. While they can sometimes lead to models converging to sharper, less generalizable minima, in this case, the larger batch sizes seemed to work well with the chosen learning rates and regularization.\n\n**Dropout Regularization**: Dropout proved crucial in preventing overfitting, especially in deeper models. Without it, the training accuracy would often reach 100% very quickly, while validation accuracy would lag significantly.\n\n**Impact of Increased Model Depth and Width**: The adjusted DeepCNN architecture with more filters and an additional dense layer in the classifier, as tested in the 'num_filters': 64, 'dropout_rate': 0.4, 'lr': 0.0008, 'batch_size': 32 parameter set, generally showed promising results, sometimes achieving higher AUC scores. This indicates that the increased capacity allowed the model to learn more complex features relevant to cancer detection.\n\n**Class Imbalance Handling**: The use of class_weights in CrossEntropyLoss was critical. Without it, the model often had lower recall for the minority 'cancerous' class, as it would disproportionately predict the 'non-cancerous' class due to its higher frequency.\n\n\n### Troubleshooting Steps Taken\n\n**Initial Overfitting**: Observed rapid overfitting in initial runs (high training accuracy, stagnant/decreasing validation accuracy).\n\n**Solution**: Implemented early stopping based on validation AUC and incorporated ReduceLROnPlateau for learning rate scheduling. Increased dropout rates and added batch normalization layers to the CNN architecture.\n\n\n**Slow Convergence**: Some initial lower learning rates or very small batch sizes led to slow convergence.\n\n**Solution**: Experimented with slightly higher initial learning rates and larger batch sizes where feasible, balanced with appropriate regularization.\n\n\n**Poor Performance on Minority Class**: Initial models struggled with identifying cancerous images.\n\n**Solution**: Introduced class_weights in the loss function to penalize misclassifications of the minority class more heavily. This improved the model's sensitivity and recall for the cancerous class.\n\n\n**Reproducibility Issue**s: Ensuring consistent results across runs.\n\n**Solution**: Set random_state for train_test_split and fixed random seeds for PyTorch and NumPy where applicable (though not explicitly shown for all components here, it's good practice for reproducibility).\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Submission","metadata":{"_uuid":"ee55c9f5-5cfc-482c-9e83-f652446f1559","_cell_guid":"d1c4c1ee-3426-40ba-bbd7-ad8c226682b8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Create submission file\nfrom torch.utils.data import DataLoader\n\nbest_result = max(results, key=lambda x: max(x['val_aucs']))  # Choose model with highest AUC\nbest_model_state = best_result['best_model']\nmodel = DeepCNN(num_filters=best_result['params']['num_filters'],\n                dropout_rate=best_result['params']['dropout_rate']).to(device)\nmodel.load_state_dict(best_model_state)\nmodel.eval()\n\ntest_df = pd.DataFrame({'id': [f.split('.tif')[0] for f in os.listdir(test_images_dir)]})\ntest_dataset = CancerDataset(test_df, test_images_dir, transform=val_transforms) # Using val_transforms for test set\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n\n# Make predictions\npredictions = []\nimage_ids = test_df['id'].values\n\nwith torch.no_grad():\n    for inputs, _ in test_loader:\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()  # Probability of class 1\n        predictions.extend(probs)\n\nsubmission_df = pd.DataFrame({'id': image_ids, 'label': predictions})\n\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submission file 'submission.csv' created successfully!\")","metadata":{"_uuid":"efa0f9d8-b870-496c-9f3f-3494201ff589","_cell_guid":"cbc29dba-e472-4bb8-ad27-de8c770132eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 7. Conclusion\n### Basic Reiterated Results\nThis project successfully developed and evaluated deep learning models for histopathologic cancer detection, addressing the binary classification problem of identifying cancerous regions in tissue images. Through systematic experimentation with different hyperparameter sets, I identified the most effective configuration based on validation AUC, which is a robust metric for imbalanced datasets.\n\nFor instance, the **best performing model** achieved a validation AUC of approximately **0.95**, demonstrating strong discriminative power between cancerous and non-cancerous images. While specific metrics varied across runs due to random initialization, the consistent high AUC across multiple configurations indicates the general effectiveness of the chosen CNN architecture and training methodology. The final submission used the model instance that yielded the highest AUC on the validation set, ensuring a well-generalized model for unseen test data.\n\n### Discussion of Learning and Takeaways\n\nSeveral key learnings emerged from this project:\n\n* **Importance of EDA**: Thorough Exploratory Data Analysis was foundational. Understanding the class imbalance (60% non-cancerous, 40% cancerous) directly led to the crucial decision of implementing **class weighting** in the loss function. Without this, the model would likely have become biased towards the majority class, resulting in poor recall for cancerous instances. Similarly, confirming uniform image dimensions (96x96 pixels) simplified the input pipeline.\n* **Data Augmentation's Role**: Implementing various data augmentations (random horizontal/vertical flips, rotations, color jitter) was vital for improving the model's generalization capabilities. Histological images often exhibit variations due to tissue preparation and scanning, and augmentation simulates these variations, making the model more robust to real-world variability.\n* **Hyperparameter Tuning Strategy**: Iterative tuning of hyperparameters (learning rate, batch size, number of filters, dropout rate) coupled with **early stopping** and a learning rate scheduler (`ReduceLROnPlateau`) was highly effective. This approach prevented overfitting and allowed the model to converge optimally, saving the best performing model state based on validation AUC. The patience parameter for early stopping proved particularly useful in stopping training once improvements plateaued, saving computational resources and preventing performance degradation from prolonged training.\n* **Model Complexity**: A deeper CNN architecture, inspired by VGG-like models but tailored for smaller image inputs, demonstrated superior feature extraction capabilities compared to simpler models. The addition of more convolutional layers and a more complex classifier with an extra dense layer helped the model capture more intricate patterns indicative of cancer.\n* **Evaluation Metrics for Imbalanced Data**: Relying solely on accuracy for evaluation would have been misleading due to class imbalance. **AUC (Area Under the Receiver Operating Characteristic Curve)** provided a much more reliable measure of the model's ability to distinguish between classes, independent of the classification threshold, which is crucial in medical diagnosis contexts. Precision, Recall, and F1-score from the classification report also offered a detailed view of performance for each class.\n### Discussion of Why Something Didnâ€™t Work\n* **Initial Overfitting**: Early iterations without proper regularization (e.g., lower dropout rates, no batch normalization) and without early stopping led to severe overfitting. The model would achieve near-perfect training accuracy but perform poorly on the validation set, indicating it was memorizing training examples rather than learning generalizable features. This was a clear sign that the model capacity exceeded the available data complexity, necessitating regularization.\n* **Suboptimal Learning Rates**: Starting with a too-high learning rate sometimes caused the model to overshoot the optimal minimum, leading to oscillating or divergent loss. Conversely, a too-low learning rate led to extremely slow convergence. The `ReduceLROnPlateau` scheduler effectively managed this by dynamically adjusting the learning rate based on validation loss, addressing the challenge of finding a static optimal learning rate.\n* **Ignoring Class Imbalance**: Initial attempts without using `class_weights` in the loss function resulted in models that had high overall accuracy but significantly lower recall for the minority (cancerous) class. This is because the model found it easier to achieve high accuracy by frequently predicting the majority class. This highlights that simply achieving high accuracy isn't sufficient for medical applications where identifying rare but critical cases is paramount.\n\n### Suggestions for Ways to Improve\n1.  **Transfer Learning**: Leveraging pre-trained models (e.g., ResNet, EfficientNet) on larger, more diverse image datasets like ImageNet could significantly boost performance. These models have learned powerful generic features that can be fine-tuned for histopathology images, requiring less data and training time.\n2.  **Advanced Data Augmentation**: Explore more sophisticated augmentation techniques such as **mixup**, **cutmix**, or **autoaugment**, which can generate more diverse training examples and potentially improve robustness further.\n3.  **Ensemble Methods**: Combining predictions from multiple models (e.g., different architectures or models trained with different random seeds) can often lead to a more robust and accurate final prediction, as ensemble methods can reduce variance and bias.\n4.  **K-Fold Cross-Validation**: Instead of a single train-validation split, using K-Fold cross-validation would provide a more robust estimate of the model's generalization performance and help in selecting the best hyperparameters, reducing the dependency on a single validation set.\n5.  **Attention Mechanisms**: Incorporating attention mechanisms into the CNN architecture could allow the model to focus on the most relevant regions of the image, potentially improving its ability to pinpoint cancerous areas more accurately.\n6.  **Gradient-weighted Class Activation Mapping (Grad-CAM)**: For better interpretability, implementing techniques like Grad-CAM could visualize which parts of the input image the CNN is focusing on when making a prediction. This would not only aid in understanding model behavior but also help in identifying potential biases or errors.\n7.  **More Extensive Hyperparameter Search**: Utilizing more advanced hyperparameter optimization libraries (e.g., Optuna, Weights & Biases Sweeps) for a more systematic and exhaustive search of the hyperparameter space could yield even better configurations. This could include a wider range of learning rates, optimizers, and architectural variations.","metadata":{"_uuid":"e9788989-94c2-4e72-92e8-4b1052997fa5","_cell_guid":"3557ca79-d303-463d-be41-31b95235aaf1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}